{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f180df-34f9-4135-b45f-278160ff97ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas Alponti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lucas Alponti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from datasets import Dataset\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbe5811e-e929-4c95-8dfd-68205c3d60e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Abrindo o objeto dataframe\n",
    "with open(r'df-pre-processado-cloud2.pickle', 'rb') as pickledfile:\n",
    "    df = pickle.load(pickledfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029fd65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo o objeto dataframe\n",
    "with open(r'df-tokenizado.pickle', 'rb') as pickledfile:\n",
    "    df2 = pickle.load(pickledfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1f7cbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 'Direito do Trabalho'),\n",
       " (1, 'Direito Penal'),\n",
       " (2, 'Direito Previdenciário'),\n",
       " (3, 'Direito Tributário'),\n",
       " (0, 'Direito Civil')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testando o mapping\n",
    "list(set([(x,y) for x,y in zip(df['label'], df2['Assunto'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71ec997-c305-4645-b0d8-59a35aac80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo o objeto dataframe\n",
    "with open(r'lista-splits.pickle', 'rb') as pickledfile:\n",
    "    lista_splits = pickle.load(pickledfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2988416a-6fda-4210-942b-0d20454e2729",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705db427-44ce-4dc0-bac3-80d9cb8c2535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c1a11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([2,1,1,1,1])\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = weights * (loss_per_sample).mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f117105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        input_ids = inputs.get(\"input_ids\")\n",
    "        outputs = model(input_ids)\n",
    "        loss = keytoken_weighted_loss(input_ids, outputs.logits, keytoken_ids)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "098dfd40-b3f2-4359-a7ce-18ac476fa802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-large-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bertmodel = AutoModelForSequenceClassification.from_pretrained('neuralmind/bert-large-portuguese-cased', num_labels = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb0721b-a5d2-493c-8013-639b36a2331c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"treino/split1\", eval_strategy=\"epoch\", use_cpu=False, no_cuda=False)\n",
    "\n",
    "t_ = lista_splits[0][1]\n",
    "v_ = lista_splits[0][2]\n",
    "\n",
    "df_teste = pd.DataFrame.from_dict(\n",
    "    {'input_ids': df.loc[t_, 'input_ids'], 'label': df.loc[t_, 'label'], 'attention_mask':df.loc[t_, 'attention_mask'] }\n",
    ")\n",
    "hg_dataset_teste = Dataset(pa.Table.from_pandas(df_teste))\n",
    "\n",
    "df_val = pd.DataFrame.from_dict(\n",
    "    {'input_ids': df.loc[v_, 'input_ids'], 'label': df.loc[v_, 'label'], 'attention_mask':df.loc[v_, 'attention_mask'] }\n",
    ")\n",
    "hg_dataset_val = Dataset(pa.Table.from_pandas(df_val))\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=bertmodel,\n",
    "    args=training_args,\n",
    "    train_dataset=hg_dataset_teste,\n",
    "    eval_dataset=hg_dataset_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c83015cc-a09a-4bd0-a1f9-360e40a09804",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25527 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4fee8-4925-4626-ade9-ea56f9911641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
