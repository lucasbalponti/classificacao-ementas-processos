{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas Alponti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lucas Alponti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "import torch\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize, linewidth=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo o objeto dataframe\n",
    "with open(r'df-tokenizado.pickle', 'rb') as pickledfile:\n",
    "    df = pickle.load(pickledfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas Alponti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "y = LabelEncoder().fit_transform(X = df['Assunto'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Modelo Random Forest - Bag of Words</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo o bag-of-words\n",
    "with open(r'bow.pickle', 'rb') as pickledfile:\n",
    "    x = pickle.load(pickledfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "cv = StratifiedKFold(n_splits = 10)\n",
    "result = cross_validate(model, x, y, cv = cv, return_estimator=True, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([809.31716967, 755.54001474, 775.33718395, 775.72851467,\n",
       "        808.54982686, 792.01552534, 787.40804529, 774.25614858,\n",
       "        744.84567404, 785.46699023]),\n",
       " 'score_time': array([11.29275179, 11.10722494, 11.09892225, 10.61786509, 10.70986724,\n",
       "        10.9108572 , 10.29583907, 10.68187094, 10.3858459 , 10.412848  ]),\n",
       " 'estimator': [RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier()],\n",
       " 'test_score': array([0.80153378, 0.83908502, 0.84966283, 0.8769007 , 0.94182203,\n",
       "        0.93203755, 0.93428534, 0.86275288, 0.75247917, 0.83670501]),\n",
       " 'train_score': array([0.99753184, 0.99665036, 0.99669443, 0.99667974, 0.99615085,\n",
       "        0.99616554, 0.99609209, 0.99663567, 0.99739962, 0.99651814])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o resultado do random-forest com bow\n",
    "with open(r'random-forest-bow.pickle', 'wb') as pickledfile:\n",
    "    pickle.dump(result, pickledfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Modelo Random Forest - TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo o tf-idf\n",
    "with open(r'tfidf.pickle', 'rb') as pickledfile:\n",
    "    x2 = pickle.load(pickledfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RandomForestClassifier()\n",
    "cv2 = StratifiedKFold(n_splits = 10)\n",
    "result2 = cross_validate(model2, x2, y, cv = cv2, return_estimator=True, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([967.43807101, 785.01799035, 789.53803682, 830.43593884,\n",
       "        834.32326269, 812.32375121, 862.92664886, 866.80971766,\n",
       "        846.82298827, 810.53587508]),\n",
       " 'score_time': array([2.96615481, 2.95326281, 3.20907474, 2.88188934, 3.19067645,\n",
       "        2.66003966, 2.67481351, 2.02461958, 3.60629368, 2.57268906]),\n",
       " 'estimator': [RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier(),\n",
       "  RandomForestClassifier()],\n",
       " 'test_score': array([0.79293931, 0.82837498, 0.84728282, 0.87266958, 0.94023536,\n",
       "        0.92674864, 0.93296311, 0.86209176, 0.7508925 , 0.83577945]),\n",
       " 'train_score': array([0.99753184, 0.99663567, 0.99669443, 0.99667974, 0.99613616,\n",
       "        0.99616554, 0.99609209, 0.99663567, 0.99739962, 0.99651814])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o resultado do random-forest com tfidf\n",
    "with open(r'random-forest-tfidf.pickle', 'wb') as pickledfile:\n",
    "    pickle.dump(result2, pickledfile, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Modelo Random Forest - BERT</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "bertmodel = BetterTransformer.transform(AutoModel.from_pretrained('neuralmind/bert-large-portuguese-cased'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 3 days, 1:14:09] |*********************     | (ETA:  14:11:06) \r"
     ]
    }
   ],
   "source": [
    "# Obtendo o último hidden state para o token [CLS]\n",
    "\n",
    "widgets = [' [',\n",
    "         progressbar.Timer(),\n",
    "         '] ',\n",
    "           progressbar.Bar('*'),' (',\n",
    "           progressbar.ETA(), ') ',\n",
    "          ]\n",
    "bar = progressbar.ProgressBar(maxval = len(df), widgets=widgets).start()\n",
    "\n",
    "# salvando para CSV, assim caso o computador desligue, o progresso não será perdido\n",
    "with open(r'embeddings.csv', 'w') as csvfile:\n",
    "  csvwriter = csv.writer(csvfile, delimiter=';', lineterminator = '\\n')\n",
    "  csvwriter.writerow(['Index', 'Embedding'])\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i in range(len(df)):\n",
    "      embeddings = bertmodel(tokenizer.encode(df['Texto tratado'][i], return_tensors='pt', max_length=512, truncation=True))['last_hidden_state'][0][0].cpu().detach().numpy()\n",
    "      # salvando para CSV, assim caso o computador desligue, o progresso não será perdido\n",
    "      csvwriter.writerow([i, embeddings])\n",
    "      bar.update(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Segunda execução] Levantando o último ID e iniciando novamente de onde parou a última execução\n",
    "with open(r'embeddings.csv') as f:\n",
    "    last_id = sum(1 for line in f) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 12:03:43] |**********************************| (ETA:  0:00:00) \r"
     ]
    }
   ],
   "source": [
    "# [Segunda execução] Obtendo o último hidden state para o token [CLS]\n",
    "\n",
    "widgets = [' [',\n",
    "         progressbar.Timer(),\n",
    "         '] ',\n",
    "           progressbar.Bar('*'),' (',\n",
    "           progressbar.ETA(), ') ',\n",
    "          ]\n",
    "bar = progressbar.ProgressBar(maxval = len(df) - (last_id+1) , widgets=widgets).start()\n",
    "\n",
    "# salvando para CSV, assim caso o computador desligue, o progresso não será perdido\n",
    "with open(r'embeddings2.csv', 'w') as csvfile:\n",
    "  csvwriter = csv.writer(csvfile, delimiter=';', lineterminator = '\\n')\n",
    "  csvwriter.writerow(['Index', 'Embedding'])\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i in range(last_id+1, len(df)):\n",
    "      embeddings = bertmodel(tokenizer.encode(df['Texto tratado'][i], return_tensors='pt', max_length=512, truncation=True))['last_hidden_state'][0][0].cpu().detach().numpy()\n",
    "      # salvando para CSV, assim caso o computador desligue, o progresso não será perdido\n",
    "      csvwriter.writerow([i, embeddings])\n",
    "      bar.update(i - last_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.concat(\n",
    "                        [\n",
    "                            pd.read_csv(r'embeddings.csv', sep=';'), \n",
    "                            pd.read_csv(r'embeddings2.csv', sep=';')\n",
    "                        ]\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.index = embeddings['Index'].values\n",
    "embeddings.drop('Index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(embeddings)):\n",
    "    row = embeddings['Embedding'][i].replace('[', '').replace(']', '').split(' ')\n",
    "    while '' in row:\n",
    "        row.remove('')\n",
    "    row = np.array([float(elem) for elem in row])\n",
    "    embeddings.loc[i, 'Embedding'] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = RandomForestClassifier()\n",
    "cv3 = StratifiedKFold(n_splits = 10)\n",
    "result3 = cross_validate(model3, embeddings['Embedding'].to_list(), y, cv = cv3, return_estimator=True, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8196482877165148"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3['test_score']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
