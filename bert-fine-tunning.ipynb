{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrindo o objeto dataframe\n",
    "with open(r'df-tokenizado.pickle', 'rb') as pickledfile:\n",
    "    df = pickle.load(pickledfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-large-portuguese-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokenizadas 2'] = [tokenizer.encode(t, truncation='only_first', padding='max_length', max_length=512) for t in df['Texto tratado']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lucas Alponti\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "y = LabelEncoder().fit_transform(X = df['Assunto'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = folds.split(df['Tokenizadas 2'], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_splits = []\n",
    "\n",
    "for f, (t_, v_) in enumerate(splits):\n",
    "    list_splits.append([f, t_, v_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-large-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bertmodel = AutoModelForSequenceClassification.from_pretrained('neuralmind/bert-large-portuguese-cased', num_labels = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"teste\", eval_strategy=\"epoch\", use_cpu=False, no_cuda=False)\n",
    "\n",
    "t_ = list_splits[0][1]\n",
    "v_ = list_splits[0][2]\n",
    "\n",
    "df_teste = pd.DataFrame.from_dict(\n",
    "    {'input_ids': df.loc[t_, 'Tokenizadas 2'], 'label': y[t_], 'attention_mask':[[0 if elem == 0 else 1 for elem in row] for row in df.loc[t_, 'Tokenizadas 2']] }\n",
    ")\n",
    "hg_dataset_teste = Dataset(pa.Table.from_pandas(df_teste))\n",
    "\n",
    "df_val = pd.DataFrame.from_dict(\n",
    "    {'input_ids': df.loc[v_, 'Tokenizadas 2'], 'label': y[v_], 'attention_mask':[[0 if elem == 0 else 1 for elem in row] for row in df.loc[v_, 'Tokenizadas 2']]}\n",
    ")\n",
    "hg_dataset_val = Dataset(pa.Table.from_pandas(df_val))\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bertmodel,\n",
    "    args=training_args,\n",
    "    train_dataset=hg_dataset_teste,\n",
    "    eval_dataset=hg_dataset_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
